{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification: Transfer Learning with ResNet\n",
    "\n",
    "In this first notebook, we implement the classification of XXX data.\n",
    "\n",
    "We'll be using a [dataset of XXX and XXX photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n",
    "\n",
    "<img src='assets/dog_cat.png'>\n",
    "\n",
    "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here the **torchvision** library ([click here for more details](https://pytorch.org/vision/stable/index.html)). This library is part of the Pytorch project, and consists of popular datasets, model architectures, and common image transformations for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation and Preparation for Transfer Learning \n",
    "\n",
    "We randomly rotate, flip and crop the input images, in order to introduce some randomness. This will further train our network, and make our predictions more robust to image transformation. \n",
    "\n",
    "We also normalize the images to make them fit to the **pre-trained** XXXnet network, and thus implement transfer learning. All pre-trained models of torchvision need to be normalized the same way (with *mean = [0.485, 0.456, 0.406]* and *std = [0.229, 0.224, 0.225]*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 695590912/2640397119 [24:36<1:01:11, 529727.60it/s]"
     ]
    }
   ],
   "source": [
    "# We define the transforms for the training data and testing data\n",
    "# The training data must fit the pre-trained model we will use for Transfer Learning\n",
    "train_transform = transforms.Compose([transforms.Resize(255),\n",
    "                                       transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.STL10(root='./data', download=True, split = 'train', transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.STL10(root='./data', download=True, split = 'test', transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to data/stl10_binary.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11536384/2640397119 [00:12<46:37, 939564.53it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/s_dwfc8s02d1vpprn90clnym0000gn/T/ipykernel_43308/1182443204.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# choose the training and test datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m train_data = datasets.STL10('data', split = 'train',\n\u001b[0m\u001b[1;32m     28\u001b[0m                               download=True, transform=train_transform)\n\u001b[1;32m     29\u001b[0m test_data = datasets.STL10('data', split = 'test',\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/site-packages/torchvision/datasets/stl10.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, folds, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset not found or corrupted. You can use download=True to download it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/site-packages/torchvision/datasets/stl10.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files already downloaded and verified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Downloading \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" to \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0m_urlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"https\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_urlretrieve\u001b[0;34m(url, filename, chunk_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUSER_AGENT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/envFOAD/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 500\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(255),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.STL10('data', split = 'train',\n",
    "                              download=True, transform=train_transform)\n",
    "test_data = datasets.STL10('data', split = 'test',\n",
    "                             download=True, transform=test_transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "validloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "put names classes STL10 !!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at a batch of images to see if they have been loaded well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 204,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # from torch to plt image format\n",
    "    plt.show()\n",
    "\n",
    "# We get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# We show some images\n",
    "imshow(utils.make_grid(images[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Implementation (Transfer Learning)\n",
    "\n",
    "We now implement our neuronal network, that will be divided into **2 parts**.\n",
    "\n",
    "The first part of our neuronal network is a **pre-trained model**: the [ResNet](https://arxiv.org/abs/1512.03385) model.\n",
    "This part of our network permits to detect the features of the images thanks to an efficient model that has been already trained on a lot of data.\n",
    "\n",
    "The second part of our neuronal network is the **classifier**, that takes the output of the ResNet as input, and classifies our images thanks to the observed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the pre-trained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We freeze the parameters of the pre-trained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Take the number of features of last layer of the pre-trained model    \n",
    "n_features = model.fc.in_features\n",
    "    \n",
    "# We write our classifier (at the position the last layer of the pre-trained model)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(n_features, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    "    )\n",
    "\n",
    "\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *Cross Entropy* loss function and the *Adam* optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "\n",
    "# We use GPU if it's available, and the CPU otherwise\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/s_dwfc8s02d1vpprn90clnym0000gn/T/ipykernel_43308/2176761905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# train the model #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# move tensors to GPU if CUDA is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "best_accuracy = 0.0\n",
    "\n",
    "epoch_all = []  # to keep the accuracies calculated at each epoch, to plot them later\n",
    "valid_epoch_acc_all = []\n",
    "train_epoch_acc_all = []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_running_corrects = 0\n",
    "    train_running_corrects = 0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for images, labels in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing images to the model\n",
    "        output = model.forward(images)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "        # to calculate the training set accuracy later: \n",
    "        # take as prediction the label with highest output (highest probability)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # total number of correct predictions\n",
    "        train_running_corrects += torch.sum(pred == labels)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for images, labels in validloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # forward pass: compute predicted outputs by passing images to the model\n",
    "        output = model.forward(images)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "        # to calculate the validation accuracy later: \n",
    "        # take as prediction the label with highest output (highest probability)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # total number of correct predictions\n",
    "        valid_running_corrects += torch.sum(pred == labels)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "    \n",
    "    # claculate accuracy\n",
    "    train_epoch_acc = train_running_corrects.double() / len(trainloader.sampler)\n",
    "    valid_epoch_acc = valid_running_corrects.double() / len(validloader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTest Accuracy: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss,valid_epoch_acc))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    # We keep the accuracies at each epoch, to plot them later, and transfer them to the cpu\n",
    "    # in order to convert the values into a numpy array and plot them\n",
    "    epoch_all.append(epoch)    \n",
    "    valid_epoch_acc_all.append(valid_epoch_acc.cpu())\n",
    "    train_epoch_acc_all.append(train_epoch_acc.cpu())\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xf/s_dwfc8s02d1vpprn90clnym0000gn/T/ipykernel_43308/594807358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmax_epoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_acc_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_acc_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"validation accuracy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinestyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'-.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#plt.plot(epochs, train_acc, label=\"train acc\", color=\"red\",linestyle=':')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_all' is not defined"
     ]
    }
   ],
   "source": [
    "max_epoch = max(epoch_all)      \n",
    "train_max_epoch_acc = max(train_epoch_acc_all)  \n",
    "valid_max_epoch_acc = max(valid_epoch_acc_all)  \n",
    "max_epoch_acc = max(train_max_epoch_acc,\n",
    "                    valid_max_epoch_acc)\n",
    "\n",
    "plt.plot(epoch_all, valid_epoch_acc_all, label=\"validation accuracy\", color=\"red\", linestyle='-.')\n",
    "plt.plot(epoch_all, train_epoch_acc_all, label=\"training accuracy\", color=\"blue\",linestyle=':')\n",
    "plt.scatter([max_epoch], [max_epoch_acc],color=\"blue\", marker=\"d\", label=\"early stopped\", s=100 )\n",
    "plt.title(\"Model w/ 2 FC layer fine tuning\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig('fctuning2[224].png',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAIRE TESTLOADER AUSSI !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Average validation accuracy (on 3 last epochs) obtained :\n",
    "\n",
    "* **ResNet18**:\n",
    "- With 0.5-normalization : 0.44\n",
    "- With Adapted-normalization : 0.44\n",
    "- With Adapted-normalization + Rotate : 0.35\n",
    "- With Adapted-normalization + Flip : 0.44\n",
    "- Same but Batch size 64 instead of 20 : 0.46\n",
    "- With Adapted-normalization + Flip + Rotate :\n",
    "- With Adapted-normalization + Flip + Rotate + Crop :\n",
    "\n",
    "* **ResNet18 without final SoftMax**:\n",
    "- With Batch size 64 + Adapted-normalization + Flip :\n",
    "\n",
    "* **ResNet 152**:\n",
    "- With Batch size 64 + Adapted-normalization + Flip : 0.517\n",
    "\n",
    "* **EfficientNet-B7**:\n",
    "\n",
    "\n",
    "* **Manual CNN of example Cifar10** (otch website):\n",
    "- With Batch size 64 + Adapted-normalization + Flip : 0.636\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING: VEDERE PERCHE NON FUNZIONAVA !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
