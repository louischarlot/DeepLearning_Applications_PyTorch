{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification\n",
    "\n",
    "In this first notebook, we implement the classification of XXX data.\n",
    "\n",
    "We'll be using a [dataset of XXX and XXX photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n",
    "\n",
    "<img src='assets/dog_cat.png'>\n",
    "\n",
    "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "import torchvision.models as models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use here the **torchvision** library ([click here for more details](https://pytorch.org/vision/stable/index.html)). This library is part of the Pytorch project, and consists of popular datasets, model architectures, and common image transformations for computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation and Preparation for Transfer Learning \n",
    "\n",
    "We randomly rotate, flip and crop the input images, in order to introduce some randomness. This will further train our network, and make our predictions more robust to image transformation. \n",
    "\n",
    "We also normalize the images to make them fit to the **pre-trained** XXXnet network, and thus implement transfer learning. All pre-trained models of torchvision need to be normalized the same way (with *mean = [0.485, 0.456, 0.406]* and *std = [0.229, 0.224, 0.225]*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# We define the transforms for the training data and testing data\n",
    "# The training data must fit the pre-trained model we will use for Transfer Learning\n",
    "train_transform = transforms.Compose([transforms.Resize(255),\n",
    "                                       transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                            [0.229, 0.224, 0.225])])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(255),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.CIFAR10(root='./data', download=True, train=True, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.CIFAR10(root='./data', download=True, train=False, transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# convert data to a normalized torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "# choose the training and test datasets\n",
    "train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "test_data = datasets.CIFAR10('data', train=False,\n",
    "                             download=True, transform=transform)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "validloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at a batch of images to see if they have been loaded well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 204,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # from torch to plt image format\n",
    "    plt.show()\n",
    "\n",
    "# We get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# We show some images\n",
    "imshow(utils.make_grid(images[0:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Implementation (Transfer Learning)\n",
    "\n",
    "We now implement our neuronal network, that will be divided into **2 parts**.\n",
    "\n",
    "The first part of our neuronal network is a **pre-trained model**: the [ResNet](https://arxiv.org/abs/1512.03385) model.\n",
    "This part of our network permits to detect the features of the images thanks to an efficient model that has been already trained on a lot of data.\n",
    "\n",
    "The second part of our neuronal network is the **classifier**, that takes the output of the ResNet as input, and classifies our images thanks to the observed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the pre-trained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We freeze the parameters of the pre-trained model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# Take the number of features of last layer of the pre-trained model    \n",
    "n_features = model.fc.in_features\n",
    "    \n",
    "# We write our classifier (at the position the last layer of the pre-trained model)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(n_features, n_features//2),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_features//2, 10),\n",
    "    nn.LogSoftmax(dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the *Cross Entropy* loss function and the *Adam* optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "\n",
    "# We use GPU if it's available, and the CPU otherwise\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "train_loss += loss.item()*data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400.0 s for each epoch\n"
     ]
    }
   ],
   "source": [
    "valid_loss = 0\n",
    "\n",
    "for images, labels in trainloader:\n",
    "        valid_loss += 1\n",
    "        \n",
    "        \n",
    "print(valid_loss*2*0.1, 's for each epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.310994 \tValidation Loss: 1.621633 \tTest Accuracy: 0.440400\n",
      "Validation loss decreased (inf --> 1.621633).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.313569 \tValidation Loss: 1.602199 \tTest Accuracy: 0.448900\n",
      "Validation loss decreased (1.621633 --> 1.602199).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.308507 \tValidation Loss: 1.626655 \tTest Accuracy: 0.442100\n",
      "Epoch: 4 \tTraining Loss: 1.312130 \tValidation Loss: 1.622995 \tTest Accuracy: 0.447600\n",
      "Epoch: 5 \tTraining Loss: 1.303058 \tValidation Loss: 1.611638 \tTest Accuracy: 0.448400\n",
      "Epoch: 6 \tTraining Loss: 1.307442 \tValidation Loss: 1.608342 \tTest Accuracy: 0.447600\n",
      "Epoch: 7 \tTraining Loss: 1.303957 \tValidation Loss: 1.667481 \tTest Accuracy: 0.424600\n",
      "Epoch: 8 \tTraining Loss: 1.299235 \tValidation Loss: 1.615641 \tTest Accuracy: 0.444500\n",
      "Epoch: 9 \tTraining Loss: 1.304413 \tValidation Loss: 1.613477 \tTest Accuracy: 0.447400\n",
      "Epoch: 10 \tTraining Loss: 1.296578 \tValidation Loss: 1.617545 \tTest Accuracy: 0.447000\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for images, labels in trainloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing images to the model\n",
    "        output = model.forward(images)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for images, labels in testloader:\n",
    "        # move tensors to GPU if CUDA is available\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        # forward pass: compute predicted outputs by passing images to the model\n",
    "        output = model.forward(images)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, labels)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*images.size(0)\n",
    "        # take as prediction the label with highest output (highest probability)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # total number of correct predictions\n",
    "        running_corrects += torch.sum(pred == labels)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(trainloader.sampler)\n",
    "    valid_loss = valid_loss/len(validloader.sampler)\n",
    "    \n",
    "    # claculate test accuracy\n",
    "    epoch_acc = running_corrects.double() / len(validloader.sampler)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f} \\tTest Accuracy: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss,epoch_acc))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING: VEDERE PERCHE NON FUNZIONAVA !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
